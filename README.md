# Real-Time Anomaly Detection Platform

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Java](https://img.shields.io/badge/Java-17+-orange.svg)](https://www.oracle.com/java/)
[![Spring Boot](https://img.shields.io/badge/Spring%20Boot-3.0+-green.svg)](https://spring.io/projects/spring-boot)
[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)

A production-grade real-time anomaly detection system designed to process 50,000+ events per minute with sub-120ms P95 latency. Built with modern microservices architecture, this platform combines event-driven streaming, machine learning inference, and distributed caching to detect anomalies at scale.

## üöÄ Key Features

- **High-Throughput Processing**: Handles 50K+ events/minute using Apache Kafka streaming
- **Low Latency**: P95 latency < 120ms for end-to-end anomaly detection
- **Machine Learning Integration**: Python-based ML inference service with real-time scoring
- **Distributed Caching**: Redis-powered feature vector caching reducing lookups by 60%+
- **Service Orchestration**: Java Spring Boot microservices for robust system coordination
- **Cross-Language Communication**: gRPC for efficient Java-Python service integration
- **Production Observability**: OpenTelemetry distributed tracing with Grafana dashboards
- **Cloud-Native Deployment**: Kubernetes (AWS EKS) for scalability and resilience

## üìä Performance Metrics

| Metric | Value |
|--------|-------|
| Event Throughput | 50,000+ events/min |
| P95 Latency | <120ms |
| Cache Hit Rate | 60%+ improvement |
| Deployment Platform | AWS EKS |
| Availability | 99.9% |

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Kafka     ‚îÇ  Event Stream
‚îÇ  Producer   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
                             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Kafka Cluster   ‚îÇ
                    ‚îÇ  (Event Bus)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Spring Boot     ‚îÇ
                    ‚îÇ  Orchestrator    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
                             ‚îÇ                 ‚îÇ
                             ‚ñº                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
                    ‚îÇ   Redis Cache    ‚îÇ       ‚îÇ
                    ‚îÇ (Feature Store)  ‚îÇ       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
                             ‚îÇ                 ‚îÇ
                             ‚ñº                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
                    ‚îÇ     gRPC         ‚îÇ       ‚îÇ
                    ‚îÇ  Communication   ‚îÇ       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
                             ‚îÇ                 ‚îÇ
                             ‚ñº                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
                    ‚îÇ  Python ML       ‚îÇ       ‚îÇ
                    ‚îÇ  Inference       ‚îÇ       ‚îÇ
                    ‚îÇ  Service         ‚îÇ       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
                             ‚îÇ                 ‚îÇ
                             ‚ñº                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
                    ‚îÇ  OpenTelemetry   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ  (Tracing)       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    Grafana       ‚îÇ
                    ‚îÇ   Dashboards     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üõ†Ô∏è Technology Stack

### Backend Services
- **Java 17+**: Primary orchestration and event processing
- **Spring Boot 3.0+**: Microservices framework
- **Python 3.9+**: ML model inference and feature engineering

### Messaging & Streaming
- **Apache Kafka**: Distributed event streaming platform
- **Kafka Streams**: Real-time stream processing

### Data Storage & Caching
- **Redis**: Distributed caching for feature vectors
- **PostgreSQL**: Metadata and configuration storage

### Communication
- **gRPC**: High-performance RPC framework for Java-Python communication
- **Protocol Buffers**: Efficient data serialization

### Machine Learning
- **Scikit-learn**: ML model training and inference
- **NumPy/Pandas**: Feature engineering and data processing

### Observability
- **OpenTelemetry**: Distributed tracing and metrics
- **Grafana**: Visualization and dashboards
- **Prometheus**: Metrics collection

### Deployment
- **Docker**: Containerization
- **Kubernetes**: Container orchestration (AWS EKS)
- **Helm**: Kubernetes package manager

## üìÅ Project Structure

```
realtime-anomaly-detection-platform/
‚îú‚îÄ‚îÄ orchestrator-service/          # Java Spring Boot orchestration
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ java/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ com/anomaly/orchestrator/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ controller/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ kafka/
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ resources/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ ml-inference-service/          # Python ML inference
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_engineering/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ grpc_server/
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ proto/                         # gRPC Protocol Buffers
‚îÇ   ‚îî‚îÄ‚îÄ anomaly_detection.proto
‚îÇ
‚îú‚îÄ‚îÄ k8s/                          # Kubernetes manifests
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ ml-service-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ kafka-deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ redis-deployment.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml
‚îÇ
‚îú‚îÄ‚îÄ monitoring/                   # Observability configs
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboards/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rules/
‚îÇ   ‚îî‚îÄ‚îÄ otel-collector-config.yaml
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml           # Local development
‚îî‚îÄ‚îÄ README.md
```

## üí° Core Features

### 1. Feature Engineering Pipeline
- **Sliding Window Aggregations**: Real-time computation of statistical features over time windows
- **Time-Series Transformations**: Lag features, rolling statistics, and trend detection
- **Feature Caching**: Redis-backed cache for frequently accessed feature vectors

### 2. ML Inference
- **Model Serving**: Python-based inference service with multiple model support
- **Real-Time Scoring**: Sub-second inference response times
- **Model Versioning**: A/B testing and gradual rollout capabilities

### 3. Event Processing
- **Kafka Consumer Groups**: Parallel processing with automatic rebalancing
- **Dead Letter Queue**: Handling of failed events with retry mechanisms
- **Exactly-Once Semantics**: Guaranteed event processing with idempotency

### 4. Observability
- **Distributed Tracing**: End-to-end request tracking across services
- **Metrics Collection**: Custom business and technical metrics
- **Alert Management**: Automated alerting on anomalies and system health

## üöÄ Getting Started

### Prerequisites

- Docker Desktop or Docker Engine 20.10+
- Kubernetes cluster (minikube for local, EKS for production)
- kubectl CLI
- Java 17+ (for local development)
- Python 3.9+ (for local development)
- Maven 3.8+

### Local Development Setup

1. **Clone the repository**
```bash
git clone https://github.com/Nikhil-Reddy25/realtime-anomaly-detection-platform.git
cd realtime-anomaly-detection-platform
```

2. **Start infrastructure services**
```bash
docker-compose up -d kafka redis postgres
```

3. **Build and run the orchestrator service**
```bash
cd orchestrator-service
mvn clean install
mvn spring-boot:run
```

4. **Set up Python environment and run ML service**
```bash
cd ml-inference-service
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python src/grpc_server/server.py
```

5. **Access the services**
- Orchestrator API: http://localhost:8080
- ML Inference Service: localhost:50051 (gRPC)
- Kafka UI: http://localhost:9000
- Grafana: http://localhost:3000

### Docker Compose Deployment

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop all services
docker-compose down
```

### Kubernetes Deployment (AWS EKS)

1. **Create EKS cluster**
```bash
export CLUSTER_NAME=anomaly-detection-prod
export REGION=us-east-1

eksctl create cluster \
  --name $CLUSTER_NAME \
  --region $REGION \
  --nodegroup-name standard-workers \
  --node-type t3.medium \
  --nodes 3 \
  --nodes-min 1 \
  --nodes-max 4
```

2. **Deploy infrastructure components**
```bash
# Install Kafka using Strimzi operator
kubectl create namespace kafka
kubectl apply -f https://strimzi.io/install/latest?namespace=kafka -n kafka
kubectl apply -f k8s/kafka-deployment.yaml -n kafka

# Deploy Redis
kubectl apply -f k8s/redis-deployment.yaml
```

3. **Deploy application services**
```bash
# Build and push Docker images
docker build -t <your-registry>/orchestrator-service:latest ./orchestrator-service
docker build -t <your-registry>/ml-inference-service:latest ./ml-inference-service

docker push <your-registry>/orchestrator-service:latest
docker push <your-registry>/ml-inference-service:latest

# Deploy to Kubernetes
kubectl apply -f k8s/orchestrator-deployment.yaml
kubectl apply -f k8s/ml-service-deployment.yaml
kubectl apply -f k8s/ingress.yaml
```

4. **Set up monitoring**
```bash
# Install Prometheus & Grafana
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm install prometheus prometheus-community/kube-prometheus-stack

# Deploy OpenTelemetry Collector
kubectl apply -f monitoring/otel-collector-config.yaml

# Import Grafana dashboards
kubectl apply -f monitoring/grafana/dashboards/
```

## üíª Usage Examples

### Sending Events to Kafka

```bash
# Using Kafka console producer
kafka-console-producer --broker-list localhost:9092 --topic anomaly-events

# Sample event
{"eventId": "evt-123", "timestamp": 1640000000, "userId": "user-456", "value": 125.5}
```

### REST API Examples

```bash
# Health check
curl http://localhost:8080/actuator/health

# Submit event for detection
curl -X POST http://localhost:8080/api/v1/detect \
  -H "Content-Type: application/json" \
  -d '{"eventId": "evt-123", "features": {"value": 125.5, "count": 10}}'

# Get detection results
curl http://localhost:8080/api/v1/results/evt-123
```

## üìä Performance Optimization

### Redis Caching Strategy
- **Feature Vector Cache**: TTL-based caching with LRU eviction
- **Warm-up Strategy**: Pre-loading frequently accessed features
- **Cache Hit Rate**: Monitored and optimized to 60%+

### Kafka Optimization
- **Partition Strategy**: Keyed by user ID for ordered processing
- **Batch Processing**: Configured for optimal throughput vs latency
- **Compression**: Snappy compression for reduced network overhead

### ML Service Optimization
- **Model Batching**: Dynamic batching of inference requests
- **Thread Pool**: Concurrent request handling
- **gRPC Streaming**: Bidirectional streaming for bulk operations

## üõ°Ô∏è Security Considerations

- **Network Policies**: Kubernetes network policies for service isolation
- **Secret Management**: AWS Secrets Manager for sensitive credentials
- **TLS Encryption**: mTLS for gRPC communication
- **Authentication**: JWT-based API authentication
- **Rate Limiting**: Per-user and per-endpoint rate limits

## üìä Monitoring & Alerting

### Key Metrics
- Event processing rate (events/sec)
- P50, P95, P99 latency percentiles
- Cache hit/miss rates
- ML inference time
- Kafka consumer lag
- Error rates by service

### Grafana Dashboards
- **System Overview**: High-level health and performance
- **Kafka Metrics**: Topic throughput, consumer lag, partition distribution
- **ML Inference**: Model performance, latency distribution
- **Cache Performance**: Hit rates, memory usage, eviction rates

### Alerts
- P95 latency > 150ms
- Consumer lag > 10,000 messages
- Cache hit rate < 40%
- Error rate > 1%
- Service unavailability

## üß™ Testing

### Unit Tests
```bash
# Java tests
cd orchestrator-service
mvn test

# Python tests
cd ml-inference-service
pytest tests/
```

### Integration Tests
```bash
# Start testcontainers environment
docker-compose -f docker-compose.test.yml up -d

# Run integration tests
mvn verify -P integration-tests
```

### Load Testing
```bash
# Using Apache JMeter
jmeter -n -t load-tests/anomaly-detection.jmx -l results.jtl

# Using k6
k6 run --vus 100 --duration 5m load-tests/spike-test.js
```

## üìà Roadmap

- [ ] Multi-model ensemble support
- [ ] Auto-scaling based on event volume
- [ ] Real-time model retraining pipeline
- [ ] Streaming feature store integration
- [ ] GraphQL API support
- [ ] Multi-region deployment
- [ ] Advanced explainability features

## üë• Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

Please ensure:
- Code follows the project style guidelines
- All tests pass
- Documentation is updated
- Commit messages are descriptive

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üìß Contact

Nikhil Reddy - [@Nikhil-Reddy25](https://github.com/Nikhil-Reddy25)

Project Link: [https://github.com/Nikhil-Reddy25/realtime-anomaly-detection-platform](https://github.com/Nikhil-Reddy25/realtime-anomaly-detection-platform)

## üôè Acknowledgments

- Apache Kafka community for the robust streaming platform
- Spring Boot team for the excellent framework
- OpenTelemetry project for observability standards
- All contributors and users of this platform

---

**Note**: This is a demonstration project showcasing system design and implementation of a high-performance anomaly detection platform. For production use, additional security hardening, compliance measures, and operational procedures should be implemented.
